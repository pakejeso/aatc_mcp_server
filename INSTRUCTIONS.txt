================================================================================
AACT Schema MCP Server — Developer Instructions
================================================================================

PROJECT OVERVIEW
----------------
This is a read-only Model Context Protocol (MCP) server that exposes the AACT
(Aggregate Analysis of ClinicalTrials.gov) database schema as MCP Resources.
It is designed so that an LLM can read the database structure and generate
accurate SQL queries, without ever executing SQL itself.

The schema is loaded from a bundled static JSON file that includes rich,
human-readable descriptions sourced from the official AACT data dictionary.
No database connection is needed.

ARCHITECTURE
------------
The server fits into a "Human-in-the-loop" SQL generation pipeline:

  User -> Backend -> LLM -> [THIS MCP SERVER reads schema] -> LLM generates SQL
                                                             -> Backend validates & executes SQL

This server ONLY provides schema context. It implements ZERO MCP Tools.

PROJECT STRUCTURE
-----------------
  aatc_mcp_server/
  ├── src/
  │   ├── __init__.py              # Package marker
  │   ├── __main__.py              # Entry point for `python -m src`
  │   └── server.py                # Main MCP server (all logic here)
  ├── data/
  │   └── aact_schema_static.json  # Bundled schema (48 tables, 63 FKs, rich descriptions)
  ├── visualizer/
  │   ├── Dockerfile               # Docker image for the flow visualizer
  │   ├── requirements.txt         # Python deps for the visualizer
  │   ├── app.py                   # Flask backend (LLM proxy)
  │   ├── index.html               # Interactive MCP flow demo UI
  │   └── schema_compact.txt       # Compact schema for LLM context
  ├── docker-compose.yml           # One-command deployment for the visualizer
  ├── pyproject.toml               # Project metadata and dependencies
  ├── .env.example                 # Template for visualizer config
  ├── .gitignore
  ├── README.md
  ├── UPDATING_SCHEMA.md           # Guide to regenerate the static schema
  ├── test_server.py               # Automated test script (9 tests)
  └── INSTRUCTIONS.txt             # This file

DEPENDENCIES
------------
  - Python 3.10+
  - mcp >= 1.0.0  (MCP SDK with FastMCP)

That's it. No database driver, no dotenv. Install: pip install -e .

MCP RESOURCES EXPOSED
---------------------
URI: postgres://aact/schema
  Returns: Complete DDL-style schema of all 48 tables (~42K chars, ~10K tokens)
  Use: Primary resource for initial LLM context

URI: postgres://aact/schema/{table_name}
  Returns: DDL for a single table + its FK relationships (both as child and parent)
  Use: On-demand detail for a specific table

URI: postgres://aact/tables
  Returns: Concise list of all tables with column counts and domain classification
  Use: Quick reference to identify relevant tables

URI: postgres://aact/relationships
  Returns: All 63 FK relationships, separated into nct_id joins and hierarchical FKs
  Use: Essential for writing correct JOINs

OUTPUT FORMAT
-------------
The schema is formatted as pseudo-DDL (CREATE TABLE statements) because LLMs
are heavily trained on SQL DDL and parse it more reliably than custom formats.
Rich descriptions are included as SQL comments.

Example output for postgres://aact/schema/conditions:

  -- Domain: Protocol | Rows per study: many
  -- Name(s) of the disease(s) or condition(s) studied in the clinical study...
  CREATE TABLE ctgov.conditions (
      nct_id character varying,  -- AACT foreign key to Study
      name character varying,
      downcase_name character varying,
      FOREIGN KEY (nct_id) REFERENCES ctgov.studies(nct_id)
  );

HOW TO RUN
----------
  # As installed script:
  aact-mcp-server

  # As Python module:
  python -m src

  # The server communicates over stdio (JSON-RPC 2.0)
  # No configuration needed — reads from the bundled static schema.

HOW TO TEST
-----------
  python test_server.py
  # Runs 9 tests covering all resources, descriptions, and schema integrity.

UPDATING THE SCHEMA
--------------------
When the AACT database schema changes, the bundled JSON needs to be regenerated.
This is a semi-automated process that requires an LLM because the rich
descriptions are not stored in the database — they must be extracted from the
AACT data dictionary (HTML) and merged with the structural data from schema.rb.

See UPDATING_SCHEMA.md for the full step-by-step guide.

KEY DESIGN DECISIONS
--------------------
1. Static-only (no DB connection): Simplifies deployment to zero configuration.
   The schema changes rarely (a few times per year at most). When it does,
   follow the UPDATING_SCHEMA.md guide to regenerate the JSON.
2. Resource-only (no Tools): Eliminates SQL injection risk entirely.
3. DDL output format: Maximizes LLM comprehension of schema structure.
4. Rich descriptions: Column and table descriptions from the official AACT
   data dictionary are embedded in the JSON, giving the LLM much better
   context than raw column names alone.
5. Per-table resources: Allows token-efficient, surgical context loading.
6. FK relationship summary: Separated into nct_id joins (46) and hierarchical
   FKs (17) so the LLM knows which JOINs are "interesting" vs. ubiquitous.

AACT DATABASE KEY FACTS
------------------------
- Schema name: ctgov
- Central table: studies (primary key: nct_id, VARCHAR)
- Most tables join to studies via nct_id
- Results tables have hierarchical FK chains:
    outcomes.id -> outcome_analyses.outcome_id
    outcome_analyses.id -> outcome_analysis_groups.outcome_analysis_id
    result_groups.id -> baseline_counts.result_group_id (and others)
- 48 tables total, ~500 columns
- Domains: Protocol (study design), Results (reported outcomes), Both

FUTURE INTEGRATION NOTES
-------------------------
This server is intended to be integrated with the AACT Analytics Website project.
When integrating:
  - The main backend should spawn this server as a subprocess (stdio transport)
  - Or use SSE transport: mcp.run(transport="sse") for HTTP-based integration
  - The LLM client connects to this server to read schema before generating SQL
  - The main backend validates and executes the generated SQL against the real DB

================================================================================
